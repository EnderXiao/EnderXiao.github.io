---
title: StarNet论文阅读
date: 2022-11-17 18:55:52
categories:
  - - 研究生
    - 科研
    - OCR
tags:
  - 手写体识别
  - 深度神经网络
  - 深度学习
  - 注意力机制
headimg:
  'https://z3.ax1x.com/2021/08/05/fego40.png'
plugins:
  - mathjax
---

## 标题
STAR-Net: A SpaTial Attention Residue Network for Scene Text Recognition

BMVC 2016

## 主要贡献

在网络中使用了位置注意力机制来消除自然图像中文本扭曲对识别的影响。

本文使用ResNet构造主干网络，结合空间注意力机制，构造了“目前最深”的端到端OCR网络

![image-20221117190850087](E:\EnderBlogSource\EnderBlog\source\images\MachineLearning\image-20221117190850087.png)

据作者所说，这是ResNet网络在场景文字识别的首次应用（？

## 方法

本文提出的网络一共包含三个部分：

1. 空间注意力机制
2. 残差特征提取器
3. CTC（Connectionist Temporal Classification）分类器

![image-20221117202424048](E:\EnderBlogSource\EnderBlog\source\images\MachineLearning\image-20221117202424048.png)

### 空间变换器

该模块的主要功能是将一个排布松散，切扭曲的文本区域，转化为一个排布紧凑规范的文本区域。

该部分总共包含三个组件:

1. localisation network
2. sampler采样器
3. interpolator插值器

![image-20221117203618331](E:\EnderBlogSource\EnderBlog\source\images\MachineLearning\image-20221117203618331.png)

在这个结构中，第一个模块用于决定输入的扭曲情况，并输出相关的变化参数。

接下来第二个模块根据第一个模块的输出参数，该模块将定位输出图像中的一些采样点，这些采样点准确的定义了被除颤的文本区域

最后，第三个模块通过对每个采样点的4领域的像素强度值进行插值，最终生成输出图像

本文为了便于解释，空间变换器使用了放射变换实现

#### Localisaion Network

对于一个宽W，高H的输入灰度图$I \in R ^{W \times H}$，输出的参数是一个仿射变换矩阵

$\theta(I) = \left [ \begin{matrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \end{matrix} \right]$

该模块采用了卷积神经网络的形式，会根据最终的损失函数学习网络的参数

#### Sampler

该层遍历上层输出的每个像素，然后定位采样点，一个输入图像的采样点$(x_i, y_i)$对应一个输出图像上的像素$(x_i', y_i')$的计算过程如下：

$\left [ \begin{matrix} x_i \\ y_i \end{matrix} \right ] = \theta (I) \left [ \begin{matrix} x_i' \\ y_i' \\ 1  \end{matrix} \right ] $

(我的理解是 这一层只不过是把上层学习出来的仿射矩阵应用到原图中X)

#### Interpolator

该层对输出图像做了四领域的双线性插值（bilinear Interpolator）

### ResNet Feature Extractor

本文使用ResNet块做特征提取，然后使用LSTM对序列特征进行解码。从CNN中输出的是带有位置信息的三维特征$C_s \times H_s \times W_s$，本文将三维的特征映射为序列特征。

本文将输出的三维特征图沿宽度进行切片，最终得到$W_s$个切片，接下来对于每个$C_s \times H_s$的切片，将他们映射为一个向量$\vec {s_t}, t = [1,2,...,W_s]$

为了避免普通卷积网络中特征丢失的问题，本文使用18个ResNet块构建网络

### LSTM

![image-20221118162146368](E:\EnderBlogSource\EnderBlog\source\images\MachineLearning\image-20221118162146368.png)

$W_i$为权重，$\vec {b}$为偏置，$\vec{h}$为隐藏状态，$\sigma$为logistic sigmoid 函数，本文使用双向LSTM层

### CTC

使用CTC进行解码无需学习参数，本文通过简化解码器，以强调特征提取器能够具有辨别性的序列特征。

最终定义的标签为$L' = L \cup \{blank\}$其中L表示一个包含26个字母和10个数字的集合。

在最后的解码阶段，使用一个softmax层来输出一个概率$y = \{ \vec{y}^1, ... , \vec{y}^{W_s} \}$表示序列特征$\vec{s}_t$在$L'$上取得的每一项概率。对于宽度为$W_s$的序列$\pi$，他的概率定义为$p(\pi) = \prod ^{W_s} _ {t=1} \vec{y}^t _{\pi _t}$

随后本文又定义了一个map to one的映射$\Beta$，用于将重复的标签和空标签移除。对于给定的标签序列$I (I.Length() \leq W_s)$，其概率公式为：

$p(I|y) = \sum _{\pi} p (\pi), \pi \in \Beta^{-1}(I)$

其中$\Beta (I)^{-1}$表示$B$的逆运算，那么CTC的损失函数即为：

$O_{ctc} = - \sum_{(I,y) \in S} ln \ p(I|y)$

其中S代表训练集。

由于序列数量会随着$W_s$指数级增长，因此本文使用了前向后向算法来计算损失函数。

## 数据集

- ICDAR-2003
- ICDAR-2013
- IIIT5K
- Street View Text
- Street View Text Perspective

![image-20221118181709744](E:\EnderBlogSource\EnderBlog\source\images\MachineLearning\image-20221118181709744.png)

![image-20221118181743686](E:\EnderBlogSource\EnderBlog\source\images\MachineLearning\image-20221118181743686.png)

![image-20221118181806815](E:\EnderBlogSource\EnderBlog\source\images\MachineLearning\image-20221118181806815.png)
