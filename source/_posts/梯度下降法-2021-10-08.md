---
title: 梯度下降法
date: 2021-10-08 12:23:24
categories:
  - - Machine Learning
    - 机器学习入门
  - - 数学工具
tags:
  - Machine Learning
  - 数学
  - 梯度下降
  - 函数极值
mathjax: true
headimg:
  'https://z3.ax1x.com/2021/08/05/fegbgU.jpg'
---

机器学习中常常需要通过求某个模型的极致来确定参数，本文讨论一下比较常用的梯度下降法
<!-- more -->

## 背景

学习线性回归算法，求解模型参数时，涉及到利用最值确定模型参数的方法。而梯度下降法求极值的方法被较多的提及，本文将梳理一下梯度下降法的思想。

## 思路

### 梯度gradient

所谓梯度，即为一个向量（矢量），用以表示某一函数在该点处的方向导数沿着该方向取得最大值。即函数在该点处沿着该方向变化最快，变化率最大（为梯度的模）

通俗来说就是爬山时，当你身处山上的某一点时，从该点出发，上山或下山的最快方向（并不考虑体力，耐力）。

### 梯度下降

梯度下降法，用梯度的思想来求解函数的**最小值**。

例如在山顶放了一个小球，松手后它会随着山坡最陡峭的地方滚落到山谷。

## 动机

就本科阶段学习的高等数学知识而言，我们也经常会接触到求函数极值的问题，我们通常会使用另倒数等于0的方法来讨论函数的极值点。例如二维`凸函数`（此处凸函数是指高数中的凹函数和凸函数的总称）

$f(x) = x^2$

通过另倒数等于0，我们可以得到如下结论:

$f'(x) = 2x = 0$

$x = 0$

即在0出取得最小值。

而对于如下这个二元函数：

$$
f(w_0,w_1) = \frac{1}{6}[ln(1+e^{w_0+2w_1}) + ln(1+e^{-w_0-7w_1})\\ + ln(1+e^{-w_0-4w_1})+ ln(1+e^{w_0+w_1})\\ + ln(1+e^{-w_0-5w_1}) + ln(1+e^{w_0+4.5w_1})]
$$

用导数法来求他的最小值会变得异常困难。而事实上，该函数是**逻辑回归的经验误差函数**，我们需要通过求这个函数的最小值来确定两个参数。

通过绘制该函数的图像，我们可以得到一个类似山谷的图像。那么我们思考能否使用梯度下降法。

## 简单例子

学习的过程通常是有简入繁的过程，接下来我们使用之前提到的相对简单的列子$f(x) = x^2$来引出梯度下降法。

### 梯度

根据梯度下降法的思路，在进行梯度下降法时，我们首先需要一个“小球”。例如，我们一开始将这个二维的”小球“放置在x=10的位置上，则小球的初始坐标为$(10,f(10)=100)$。

”小球“由于受力不平衡，那么他会向山坡下移动，一些物理学知识我们可以预测，小球将会向坡度最陡峭的那个方向移动，这就与梯度的概念不谋而合。由于此处我们需要讨论向量，为了方便，我们规定标量+括号，如：$(x)$表示一个向量

于是我们求该点处的梯度：

$\Delta f(x_0) = f'(x_0)(i) = (f'(x_0)) = (2x|_{x_0 = 10}) = (20)$

可见在二位坐标系下，梯度向量是一个以为向量，反应到坐标上的变化，也就是x的变化率。

那么我们将“小球”所在的位置加上梯度方向，则就可以得到小球移动后的向量$(x_1)$：

$(x_1) = (x_0) - \eta(\Delta f(x_0))$

其中$\eta$成为步长，用于控制每次滚动的距离标量。此处我们使用$\eta = 0.2$，那么：

$(x_1) = (x_0) - 0.2\times (\Delta f(x_0)) = (6)$

### 迭代

至此我们完成了”小球”的一步滚动，但是要完整实现我们的思路，还需要使小球连续运动直到它到达谷底。一个很常规的思路就是得到每一步的终点后，以该点为基础继续进行移动，也就是迭代。

$x_1$的梯度为：

$\Delta f(x_1) = f'(x_1)(i) = (f'(x_1)) = (2x|_{x_1=6}) = (12)$

则下一步“小球”将移动到：

$(x_2) = (x_1) - \eta \Delta f(x_1) = (6) - 0.2 \times (12) = (3.6)$

在迭代到第10次时，我们得到$x_{10} \approx 0$

### 总结

此时我们把每次求得的梯度的模长$||\Delta f||$，归纳起来，可以得到下面的表格：

|                 | $x_0$ | $x_1$ | $x_2$ | $x_3$ | $x_4$ | $x_5$ | $x_6$ | $x_7$ | $x_8$ | $x_9$ | $x_{10}$ |
| --------------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | -------- |
| \|\| $\Delta  f$ \|\| | 20    | 12    | 7.2   | 4.32  | 2.59  | 1.56  | 0.93  | 0.56  | 0.34  | 0.2   | 0.12     |

观察可以发现，随着迭代次数的增加有：

$||\Delta  f|| = 0 \to \Delta f = 0 \to f'(x) = 0$

最终我们得到与另倒数等于0的这种方法相似的效果。但同样可以知道使用此法测算出的结果是一个接近于极值点的情况，并不是真正的极值点。这也体现了我们在将理论进行实际引应用中，为了解决种种问题而在最终结果上做出妥协的思考方式。

### 关于步长

上文提到，步长可以由我们自由设置，但是如果步长设置不够合理，将会导致无法找到最大或最小值的情况。

例如在测算$f(x) = x^2$时，令$\eta = 1.1$，那么随着迭代的增加$f(x)$的值会越来越大。换句话说，就是小球某一步直接滚过了最小值，导致小球在无摩擦力的情况下每次获得了更大的重力势能，导致其越滚越高。

通常步长不当可根据$f(x)$的变化趋势，分为以下三种情况：

1. $f(x)$越来越大，则$\eta$过大
2. $f(x)$一开始急剧下降，到达某一值时几乎不变，则$\eta$较大
3. $f(x)$几乎是线性变化，即变化过于缓慢，则$\eta$较小，可适当提高

## 总结

至此，梯度下降法的基本思想和操作方式讲解已经结束了，为我们提供了一种新的求极值的思路，并且让我们回顾了科学技术在实际应用中的适应思路。之后需要多多练习。对于更高纬的情况，例如:

$f(x) = x_1^2 + 2x_2^2$

可以使用梯度下降法自行求解。

